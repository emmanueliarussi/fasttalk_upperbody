{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf14c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal imports for SMPL-X -> vertices -> Renderer (headless-friendly)\n",
    "import os\n",
    "# change base folder\n",
    "os.chdir('../')\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import pickle\n",
    "import smplx\n",
    "from renderer.renderer import Renderer\n",
    "from renderer.util import batch_orth_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bc04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/mnt/GUAVA/assets/example/tracked_video/6gvP8f5WQyo__056/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7706d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#shape_path = base_path + \"id_share_params.pkl\"\n",
    "\n",
    "#with open(shape_path, \"rb\") as f:\n",
    "#    data = pickle.load(f)\n",
    "\n",
    "\n",
    "#print(\"Available keys:\", list(data.keys()))\n",
    "\n",
    "#for key, value in data.items():\n",
    "#    if isinstance(value, np.ndarray):\n",
    "#        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "#    else:\n",
    "#        print(f\"{key}: type={type(value)} -> {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f3c6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['frame_000000', 'frame_000001', 'frame_000002', 'frame_000003', 'frame_000004', 'frame_000005', 'frame_000006', 'frame_000007', 'frame_000008', 'frame_000009', 'frame_000010', 'frame_000011', 'frame_000012', 'frame_000013', 'frame_000014', 'frame_000015', 'frame_000016', 'frame_000017', 'frame_000018', 'frame_000019', 'frame_000020', 'frame_000021', 'frame_000022', 'frame_000023', 'frame_000024', 'frame_000025', 'frame_000026', 'frame_000027', 'frame_000028', 'frame_000029', 'frame_000030', 'frame_000031', 'frame_000032', 'frame_000033', 'frame_000034', 'frame_000035', 'frame_000036', 'frame_000037', 'frame_000038', 'frame_000039', 'frame_000040', 'frame_000041', 'frame_000042', 'frame_000043', 'frame_000044', 'frame_000045', 'frame_000046', 'frame_000047', 'frame_000048', 'frame_000049', 'frame_000050', 'frame_000051', 'frame_000052', 'frame_000053', 'frame_000054', 'frame_000055', 'frame_000056', 'frame_000057', 'frame_000058', 'frame_000059', 'frame_000060', 'frame_000061', 'frame_000062', 'frame_000063', 'frame_000064', 'frame_000065', 'frame_000066', 'frame_000067', 'frame_000068', 'frame_000069', 'frame_000070', 'frame_000071', 'frame_000072', 'frame_000073', 'frame_000074', 'frame_000075', 'frame_000076', 'frame_000077', 'frame_000078', 'frame_000079', 'frame_000080', 'frame_000081', 'frame_000082', 'frame_000083', 'frame_000084', 'frame_000085', 'frame_000086', 'frame_000087', 'frame_000088', 'frame_000089', 'frame_000090', 'frame_000091', 'frame_000092', 'frame_000093', 'frame_000094', 'frame_000095', 'frame_000096', 'frame_000097', 'frame_000098', 'frame_000099', 'frame_000100', 'frame_000101', 'frame_000102', 'frame_000103', 'frame_000104', 'frame_000105', 'frame_000106', 'frame_000107', 'frame_000108', 'frame_000109', 'frame_000110', 'frame_000111', 'frame_000112', 'frame_000113', 'frame_000114', 'frame_000115', 'frame_000116', 'frame_000117', 'frame_000118', 'frame_000119', 'frame_000120', 'frame_000121', 'frame_000122', 'frame_000123', 'frame_000124', 'frame_000125', 'frame_000126', 'frame_000127', 'frame_000128', 'frame_000129', 'frame_000130', 'frame_000131', 'frame_000132', 'frame_000133', 'frame_000134', 'frame_000135', 'frame_000136', 'frame_000137', 'frame_000138', 'frame_000139', 'frame_000140', 'frame_000141', 'frame_000142', 'frame_000143', 'frame_000144', 'frame_000145', 'frame_000146', 'frame_000147', 'frame_000148', 'frame_000149', 'frame_000150', 'frame_000151', 'frame_000152', 'frame_000153', 'frame_000154', 'frame_000155', 'frame_000156', 'frame_000157', 'frame_000158', 'frame_000159', 'frame_000160', 'frame_000161', 'frame_000162', 'frame_000163', 'frame_000164', 'frame_000165', 'frame_000166', 'frame_000167', 'frame_000168', 'frame_000169', 'frame_000170', 'frame_000171', 'frame_000172', 'frame_000173', 'frame_000174', 'frame_000175', 'frame_000176', 'frame_000177', 'frame_000178', 'frame_000179', 'frame_000180', 'frame_000181', 'frame_000182', 'frame_000183', 'frame_000184', 'frame_000185', 'frame_000186', 'frame_000187', 'frame_000188', 'frame_000189', 'frame_000190', 'frame_000191', 'frame_000192', 'frame_000193', 'frame_000194', 'frame_000195', 'frame_000196', 'frame_000197', 'frame_000198', 'frame_000199', 'frame_000200', 'frame_000201', 'frame_000202', 'frame_000203', 'frame_000204', 'frame_000205', 'frame_000206', 'frame_000207', 'frame_000208', 'frame_000209', 'frame_000210', 'frame_000211', 'frame_000212', 'frame_000213', 'frame_000214', 'frame_000215', 'frame_000216', 'frame_000217', 'frame_000218', 'frame_000219', 'frame_000220', 'frame_000221', 'frame_000222', 'frame_000223', 'frame_000224', 'frame_000225', 'frame_000226', 'frame_000227', 'frame_000228', 'frame_000229', 'frame_000230', 'frame_000231', 'frame_000232', 'frame_000233', 'frame_000234', 'frame_000235', 'frame_000236', 'frame_000237', 'frame_000238', 'frame_000239', 'frame_000240', 'frame_000241', 'frame_000242', 'frame_000243', 'frame_000244', 'frame_000245', 'frame_000246', 'frame_000247', 'frame_000248', 'frame_000249', 'frame_000250', 'frame_000251', 'frame_000252', 'frame_000253', 'frame_000254', 'frame_000255', 'frame_000256', 'frame_000257', 'frame_000258', 'frame_000259', 'frame_000260', 'frame_000261', 'frame_000262', 'frame_000263', 'frame_000264', 'frame_000265', 'frame_000266', 'frame_000267', 'frame_000268', 'frame_000269', 'frame_000270', 'frame_000271', 'frame_000272', 'frame_000273', 'frame_000274', 'frame_000275', 'frame_000276', 'frame_000277', 'frame_000278', 'frame_000279', 'frame_000280', 'frame_000281', 'frame_000282', 'frame_000283', 'frame_000284', 'frame_000285', 'frame_000286', 'frame_000287', 'frame_000288', 'frame_000289', 'frame_000290', 'frame_000291', 'frame_000292', 'frame_000293', 'frame_000294', 'frame_000295', 'frame_000296', 'frame_000297', 'frame_000298', 'frame_000299', 'frame_000300', 'frame_000301', 'frame_000302', 'frame_000303', 'frame_000304', 'frame_000305', 'frame_000306', 'frame_000307', 'frame_000308', 'frame_000309', 'frame_000310', 'frame_000311', 'frame_000312', 'frame_000313', 'frame_000314', 'frame_000315', 'frame_000316', 'frame_000317', 'frame_000318', 'frame_000319', 'frame_000320', 'frame_000321', 'frame_000322', 'frame_000323', 'frame_000324', 'frame_000325', 'frame_000326', 'frame_000327', 'frame_000328', 'frame_000329', 'frame_000330', 'frame_000331', 'frame_000332', 'frame_000333', 'frame_000334', 'frame_000335', 'frame_000336', 'frame_000337', 'frame_000338', 'frame_000339', 'frame_000340', 'frame_000341', 'frame_000342', 'frame_000343', 'frame_000344', 'frame_000345', 'frame_000346', 'frame_000347', 'frame_000348', 'frame_000349', 'frame_000350', 'frame_000351', 'frame_000352', 'frame_000353', 'frame_000354', 'frame_000355', 'frame_000356', 'frame_000357', 'frame_000358', 'frame_000359', 'frame_000360', 'frame_000361', 'frame_000362', 'frame_000363', 'frame_000364', 'frame_000365', 'frame_000366', 'frame_000367', 'frame_000368', 'frame_000369', 'frame_000370', 'frame_000371', 'frame_000372', 'frame_000373', 'frame_000374', 'frame_000375', 'frame_000376', 'frame_000377', 'frame_000378', 'frame_000379', 'frame_000380', 'frame_000381', 'frame_000382', 'frame_000383', 'frame_000384', 'frame_000385', 'frame_000386', 'frame_000387', 'frame_000388', 'frame_000389', 'frame_000390', 'frame_000391', 'frame_000392', 'frame_000393', 'frame_000394', 'frame_000395', 'frame_000396', 'frame_000397', 'frame_000398', 'frame_000399', 'frame_000400', 'frame_000401', 'frame_000402', 'frame_000403', 'frame_000404', 'frame_000405', 'frame_000406', 'frame_000407', 'frame_000408', 'frame_000409', 'frame_000410', 'frame_000411', 'frame_000412', 'frame_000413', 'frame_000414', 'frame_000415', 'frame_000416', 'frame_000417', 'frame_000418', 'frame_000419', 'frame_000420', 'frame_000421', 'frame_000422', 'frame_000423', 'frame_000424'])\n",
      "dict_keys(['body_crop', 'dwpose_raw', 'dwpose_rlt', 'smplx_coeffs', 'head_crop', 'head_lmk_203', 'head_lmk_70', 'head_lmk_mp', 'flame_coeffs', 'left_mano_coeffs', 'left_hand_crop', 'right_mano_coeffs', 'right_hand_crop'])\n"
     ]
    }
   ],
   "source": [
    "tracking_path = base_path + \"optim_tracking_ehm.pkl\"\n",
    "\n",
    "with open(tracking_path, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Print the keys of the dictionary to see what it contains\n",
    "print(data.keys())\n",
    "print(data['frame_000000'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a53ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in frame: dict_keys(['body_crop', 'dwpose_raw', 'dwpose_rlt', 'smplx_coeffs', 'head_crop', 'head_lmk_203', 'head_lmk_70', 'head_lmk_mp', 'flame_coeffs', 'left_mano_coeffs', 'left_hand_crop', 'right_mano_coeffs', 'right_hand_crop'])\n",
      "\n",
      "Keys in smplx_coeffs: dict_keys(['exp', 'global_pose', 'body_pose', 'body_cam', 'camera_RT_params', 'left_hand_pose', 'right_hand_pose'])\n",
      "\n",
      "=== Detailed structure of smplx_coeffs ===\n",
      "exp: shape=(50,), dtype=float32, first few values: [1.2100561  0.5304717  0.11870743 0.19686127 0.08652326]\n",
      "global_pose: shape=(3,), dtype=float32, first few values: [ 2.9868624   0.06696548 -0.25063005]\n",
      "body_pose: shape=(21, 3), dtype=float32, first few values: [-0.05537486  0.08022741  0.00862682 -0.0110795  -0.00271257]\n",
      "body_cam: shape=(3,), dtype=float32, first few values: [1.9841318  0.05343538 0.83356845]\n",
      "camera_RT_params: shape=(3, 4), dtype=float32, first few values: [-0.9999938  -0.00333516  0.00106661 -0.04895249  0.00326747]\n",
      "left_hand_pose: shape=(15, 3), dtype=float32, first few values: [-0.08508328  0.12008699 -1.3282686   0.20950751 -0.20685533]\n",
      "right_hand_pose: shape=(15, 3), dtype=float32, first few values: [ 0.00409186 -0.321227    0.8929021   0.41290188  0.21008031]\n"
     ]
    }
   ],
   "source": [
    "# Check what's actually in the nested structure\n",
    "sample_frame = data['frame_000000']\n",
    "print(\"Keys in frame:\", sample_frame.keys())\n",
    "print(\"\\nKeys in smplx_coeffs:\", sample_frame['smplx_coeffs'].keys())\n",
    "print(\"\\n=== Detailed structure of smplx_coeffs ===\")\n",
    "for key, value in sample_frame['smplx_coeffs'].items():\n",
    "    if isinstance(value, np.ndarray):\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}, first few values: {value.flatten()[:5]}\")\n",
    "    else:\n",
    "        print(f\"{key}: type={type(value)} -> {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15e2ea33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape file not found, using default neutral betas\n",
      "Betas provided: 10\n",
      "Frames: 425\n",
      "Found dims -> expr:50 body:63\n",
      "Model expects -> betas:10 expr:10 hands:45 body:63\n",
      "Model expects -> betas:10 expr:10 hands:45 body:63\n",
      "Init ready: verts0 (1, 10475, 3) scale 1.6801521579389072\n",
      "Init ready: verts0 (1, 10475, 3) scale 1.6801521579389072\n"
     ]
    }
   ],
   "source": [
    "# Build SMPL-X model, parse coeffs, and prep camera/render context (no PCA hands, keep 45D)\n",
    "from pathlib import Path\n",
    "\n",
    "# Files\n",
    "shape_path = base_path + 'id_share_params.pkl'\n",
    "tracking_path = base_path + 'optim_tracking_ehm.pkl'\n",
    "\n",
    "# Load shape data if available, otherwise use defaults\n",
    "if Path(shape_path).exists():\n",
    "    with open(shape_path, 'rb') as f:\n",
    "        shape_data = pickle.load(f)\n",
    "    betas_np = np.asarray(shape_data.get('smplx_shape'))  # often (1, 200)\n",
    "    betas_np = betas_np.reshape(1, -1) if betas_np.ndim == 1 else betas_np\n",
    "    print('Loaded betas from file')\n",
    "else:\n",
    "    # Use default neutral shape (zeros)\n",
    "    betas_np = np.zeros((1, 10), dtype=np.float32)\n",
    "    print('Shape file not found, using default neutral betas')\n",
    "\n",
    "betas_full = torch.from_numpy(betas_np).float()\n",
    "print('Betas provided:', betas_full.shape[1])\n",
    "\n",
    "# Load tracking data\n",
    "with open(tracking_path, 'rb') as f:\n",
    "    tracking = pickle.load(f)\n",
    "\n",
    "frame_keys = sorted([k for k in tracking.keys() if k.startswith('frame_')])\n",
    "assert len(frame_keys) > 0, 'No frames found in tracking PKL'\n",
    "print('Frames:', len(frame_keys))\n",
    "\n",
    "# Inspect first frame to determine expression dim\n",
    "def get_inner_coeffs(fd):\n",
    "    return fd['smplx_coeffs'] if isinstance(fd, dict) and 'smplx_coeffs' in fd else fd\n",
    "\n",
    "sample = get_inner_coeffs(tracking[frame_keys[0]])\n",
    "\n",
    "def get_len(d, keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            v = np.asarray(d[k]).reshape(-1)\n",
    "            return v.shape[0]\n",
    "    return default\n",
    "\n",
    "expr_dim_in = get_len(sample, ['expression','expr'], 50)\n",
    "body_dim_in = get_len(sample, ['body_pose','body','pose_body'], 63)\n",
    "print(f'Found dims -> expr:{expr_dim_in} body:{body_dim_in}')\n",
    "\n",
    "# Create SMPL-X model (always axis-angle hands 45D, keep it simple)\n",
    "smplx_model_dir = '/mnt/fasttalk_upperbody/'  # user-specified\n",
    "assert Path(smplx_model_dir).exists(), f'Missing SMPLX assets at {smplx_model_dir}'\n",
    "model = smplx.create(\n",
    "    smplx_model_dir,\n",
    "    model_type='smplx', gender='neutral', ext='npz',\n",
    "    num_betas=10,             # SMPL-X default\n",
    "    num_expression_coeffs=10, # SMPL-X default\n",
    "    use_pca=False,            # no PCA hands\n",
    "    flat_hand_mean=True       # use flat (open) hand mean for natural opening\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# Model-accepted sizes\n",
    "n_betas_model = model.num_betas\n",
    "n_exp_model   = model.num_expression_coeffs\n",
    "print(f'Model expects -> betas:{n_betas_model} expr:{n_exp_model} hands:45 body:63')\n",
    "\n",
    "# Conform betas to model\n",
    "betas_used = betas_full[:, :n_betas_model] if betas_full.shape[1] >= n_betas_model else torch.cat([betas_full, torch.zeros(1, n_betas_model-betas_full.shape[1])], dim=1)\n",
    "\n",
    "# Faces (triangles)\n",
    "faces = np.load('/mnt/fasttalk_upperbody/smplx/smplx_faces.npy').astype(np.int64)  # user-specified\n",
    "faces_t = torch.from_numpy(faces)[None, ...].to(device)\n",
    "\n",
    "def to_row_tensor(arr):\n",
    "    if arr is None:\n",
    "        return None\n",
    "    t = torch.from_numpy(np.asarray(arr).reshape(-1)).float()\n",
    "    return t[None, :]\n",
    "\n",
    "def fit_dim_row(t, target):\n",
    "    if t is None:\n",
    "        return torch.zeros(1, target).float()\n",
    "    if t.shape[1] > target:\n",
    "        return t[:, :target]\n",
    "    if t.shape[1] < target:\n",
    "        return torch.cat([t, torch.zeros(1, target - t.shape[1], dtype=t.dtype)], dim=1)\n",
    "    return t\n",
    "\n",
    "def first_val(d, keys):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return None\n",
    "\n",
    "def parse_frame(fd):\n",
    "    # Align with project data_loader: use only smplx_coeffs entries, no MANO fallbacks\n",
    "    d = get_inner_coeffs(fd)\n",
    "\n",
    "    # Global/body/face\n",
    "    go  = to_row_tensor(first_val(d, ['global_pose','global_orient','root_orient','orient']))\n",
    "    bp  = to_row_tensor(first_val(d, ['body_pose','body','pose_body']))\n",
    "    jp  = to_row_tensor(first_val(d, ['jaw_pose','jaw']))\n",
    "    lep = to_row_tensor(first_val(d, ['leye_pose','left_eye_pose']))\n",
    "    rep = to_row_tensor(first_val(d, ['reye_pose','right_eye_pose']))\n",
    "\n",
    "    # Hands: strictly use SMPL-X hand pose from smplx_coeffs (axis-angle, 45D)\n",
    "    lhp = to_row_tensor(d.get('left_hand_pose'))\n",
    "    rhp = to_row_tensor(d.get('right_hand_pose'))\n",
    "\n",
    "    exp = to_row_tensor(first_val(d, ['exp','expression','expr']))\n",
    "    trn = to_row_tensor(first_val(d, ['transl','translation','trans']))\n",
    "\n",
    "    # Fit dims (keep as flat row tensors, SMPL-X expects this)\n",
    "    go  = fit_dim_row(go, 3)                         # (1,3)\n",
    "    bp  = fit_dim_row(bp, 63)                        # (1,63)\n",
    "    jp  = fit_dim_row(jp, 3)                         # (1,3)\n",
    "    lep = fit_dim_row(lep, 3)                        # (1,3)\n",
    "    rep = fit_dim_row(rep, 3)                        # (1,3)\n",
    "    lhp = fit_dim_row(lhp, 45)                       # (1,45)\n",
    "    rhp = fit_dim_row(rhp, 45)                       # (1,45)\n",
    "    exp = fit_dim_row(exp, n_exp_model)              # (1,n_exp)\n",
    "    trn = fit_dim_row(trn, 3)                        # (1,3)\n",
    "    return {\n",
    "        'global_orient': go,\n",
    "        'body_pose': bp,\n",
    "        'jaw_pose': jp,\n",
    "        'leye_pose': lep,\n",
    "        'reye_pose': rep,\n",
    "        'left_hand_pose': lhp,\n",
    "        'right_hand_pose': rhp,\n",
    "        'expression': exp,\n",
    "        'transl': trn,\n",
    "    }\n",
    "\n",
    "# First frame: vertices + camera frame (orthographic)\n",
    "first = parse_frame(tracking[frame_keys[0]])\n",
    "with torch.no_grad():\n",
    "    out = model(\n",
    "        betas=betas_used.to(device),\n",
    "        global_orient=first['global_orient'].to(device),\n",
    "        body_pose=first['body_pose'].to(device),\n",
    "        jaw_pose=first['jaw_pose'].to(device),\n",
    "        leye_pose=first['leye_pose'].to(device),\n",
    "        reye_pose=first['reye_pose'].to(device),\n",
    "        left_hand_pose=first['left_hand_pose'].to(device),\n",
    "        right_hand_pose=first['right_hand_pose'].to(device),\n",
    "        expression=first['expression'].to(device),\n",
    "        transl=first['transl'].to(device),\n",
    "    )\n",
    "verts0_t = out.vertices  # [1, V, 3]\n",
    "\n",
    "# Center and set a fixed scale for the whole clip\n",
    "center_t = verts0_t.mean(dim=1, keepdim=True)\n",
    "verts0_centered = verts0_t - center_t\n",
    "scale = float(2.1 / (verts0_centered.abs().max().item() + 1e-6))\n",
    "cam_params = torch.tensor([[scale, 0.0, 0.0]], dtype=torch.float32, device=device)\n",
    "print('Init ready: verts0', tuple(verts0_t.shape), 'scale', scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc6c868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model internal dimensions ===\n",
      "shapedirs: torch.Size([10475, 3, 10])\n",
      "expr_dirs: torch.Size([10475, 3, 10])\n",
      "num_betas: 10\n",
      "num_expression_coeffs: 10\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Debug: print model internal dimensions\n",
    "print('=== Model internal dimensions ===')\n",
    "print(f'shapedirs: {model.shapedirs.shape}')\n",
    "print(f'expr_dirs: {model.expr_dirs.shape}')\n",
    "print(f'num_betas: {model.num_betas}')\n",
    "print(f'num_expression_coeffs: {model.num_expression_coeffs}')\n",
    "print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95212ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10475, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verts0_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f229a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left hand pose (len): 45 first 6: tensor([-0.0851,  0.1201, -1.3283,  0.2095, -0.2069, -0.6650])\n",
      "Right hand pose (len): 45 first 6: tensor([ 0.0041, -0.3212,  0.8929,  0.4129,  0.2101,  1.1837])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check hands after parser simplification\n",
    "fp = parse_frame(tracking[frame_keys[0]])\n",
    "print('Left hand pose (len):', fp['left_hand_pose'].shape[1], 'first 6:', fp['left_hand_pose'][0, :6])\n",
    "print('Right hand pose (len):', fp['right_hand_pose'].shape[1], 'first 6:', fp['right_hand_pose'][0, :6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f2696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAME dims -> shape:100 exp:50\n",
      "WARN: Could not parse SMPL-X__FLAME mapping; disabling fusion.\n"
     ]
    }
   ],
   "source": [
    "# Setup FLAME model and FLAME->SMPL-X head fusion mapping\n",
    "from flame_model.FLAME import FLAMEModel\n",
    "\n",
    "# Detect FLAME dims from the first frame\n",
    "flame_sample = tracking[frame_keys[0]].get('flame_coeffs', {})\n",
    "def _get_len_np(d, keys, default=None):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            v = np.asarray(d[k]).reshape(-1)\n",
    "            return v.shape[0]\n",
    "    return default\n",
    "\n",
    "flame_n_shape = _get_len_np(flame_sample, ['shape_params','shape','betas'], 100)\n",
    "flame_n_exp   = _get_len_np(flame_sample, ['expression','exp','expression_params'], 50)\n",
    "print(f'FLAME dims -> shape:{flame_n_shape} exp:{flame_n_exp}')\n",
    "\n",
    "# Create FLAME model (no landmarks needed for rendering)\n",
    "flame = FLAMEModel(n_shape=flame_n_shape, n_exp=flame_n_exp, no_lmks=True).to(device).eval()\n",
    "\n",
    "# Load mapping from FLAME vertices to SMPL-X vertices\n",
    "fusion_enabled = True\n",
    "map_path = '/mnt/fasttalk_upperbody/smplx/SMPL-X__FLAME_vertex_ids.npy'\n",
    "try:\n",
    "    mapping_raw = np.load(map_path, allow_pickle=True)\n",
    "    smplx_idx = None\n",
    "    flame_idx = None\n",
    "    # Accept a variety of formats\n",
    "    if isinstance(mapping_raw, np.ndarray) and mapping_raw.dtype != object:\n",
    "        if mapping_raw.ndim == 2 and mapping_raw.shape[1] == 2:\n",
    "            smplx_idx = mapping_raw[:, 0].astype(np.int64)\n",
    "            flame_idx = mapping_raw[:, 1].astype(np.int64)\n",
    "    elif isinstance(mapping_raw, np.ndarray) and mapping_raw.dtype == object:\n",
    "        obj = mapping_raw.item() if mapping_raw.size == 1 else mapping_raw\n",
    "        # Try common keys\n",
    "        for k_smpl, k_fla in [('smplx', 'flame'), ('smplx_idx','flame_idx'), ('smplx_ids','flame_ids')]:\n",
    "            if isinstance(obj, dict) and (k_smpl in obj) and (k_fla in obj):\n",
    "                smplx_idx = np.asarray(obj[k_smpl]).astype(np.int64)\n",
    "                flame_idx = np.asarray(obj[k_fla]).astype(np.int64)\n",
    "                break\n",
    "    if smplx_idx is None or flame_idx is None or len(smplx_idx) != len(flame_idx):\n",
    "        print('WARN: Could not parse SMPL-X__FLAME mapping; disabling fusion.')\n",
    "        fusion_enabled = False\n",
    "    else:\n",
    "        smplx_idx_t = torch.from_numpy(smplx_idx).long().to(device)\n",
    "        flame_idx_t = torch.from_numpy(flame_idx).long().to(device)\n",
    "        print(f'FLAME head fusion mapping loaded: {len(smplx_idx)} vertices')\n",
    "except Exception as e:\n",
    "    print('WARN: Failed to load mapping file:', e)\n",
    "    fusion_enabled = False\n",
    "\n",
    "def to_row_tensor_np(arr):\n",
    "    if arr is None: return None\n",
    "    t = torch.from_numpy(np.asarray(arr).reshape(-1)).float()\n",
    "    return t[None, :]\n",
    "\n",
    "def fit_dim_row_like(t, target):\n",
    "    if t is None:\n",
    "        return torch.zeros(1, target).float()\n",
    "    if t.shape[1] > target:\n",
    "        return t[:, :target]\n",
    "    if t.shape[1] < target:\n",
    "        return torch.cat([t, torch.zeros(1, target - t.shape[1], dtype=t.dtype)], dim=1)\n",
    "    return t\n",
    "\n",
    "def first_val_dict(d, keys):\n",
    "    for k in keys:\n",
    "        if k in d and d[k] is not None:\n",
    "            return d[k]\n",
    "    return None\n",
    "\n",
    "def parse_flame(fd):\n",
    "    d_smplx = get_inner_coeffs(fd)\n",
    "    d_flame = fd.get('flame_coeffs', {}) if isinstance(fd, dict) else {}\n",
    "    \n",
    "    shp = to_row_tensor_np(first_val_dict(d_flame, ['shape_params','shape','betas']))\n",
    "    exp = to_row_tensor_np(first_val_dict(d_flame, ['expression','exp','expression_params']))\n",
    "    jaw = to_row_tensor_np(first_val_dict(d_flame, ['jaw_pose','jaw']))\n",
    "    # Eyes: prefer FLAME if present, else fallback to SMPL-X\n",
    "    lep = to_row_tensor_np(first_val_dict(d_flame, ['leye_pose','left_eye_pose']))\n",
    "    rep = to_row_tensor_np(first_val_dict(d_flame, ['reye_pose','right_eye_pose']))\n",
    "    if lep is None: lep = to_row_tensor_np(first_val(d_smplx, ['leye_pose','left_eye_pose']))\n",
    "    if rep is None: rep = to_row_tensor_np(first_val(d_smplx, ['reye_pose','right_eye_pose']))\n",
    "    # Global orient: reuse body global from SMPL-X to stay consistent\n",
    "    go  = to_row_tensor_np(first_val(d_smplx, ['global_pose','global_orient','root_orient','orient']))\n",
    "    \n",
    "    # Fit dims\n",
    "    shp = fit_dim_row_like(shp, flame_n_shape)\n",
    "    exp = fit_dim_row_like(exp, flame_n_exp)\n",
    "    jaw = fit_dim_row_like(jaw, 3)\n",
    "    lep = fit_dim_row_like(lep, 3)\n",
    "    rep = fit_dim_row_like(rep, 3)\n",
    "    go  = fit_dim_row_like(go, 3)\n",
    "    \n",
    "    pose = torch.cat([go, jaw], dim=1)   # (1,6)\n",
    "    eye  = torch.cat([lep, rep], dim=1)  # (1,6)\n",
    "    return {\n",
    "        'shape_params': shp,\n",
    "        'expression_params': exp,\n",
    "        'pose_params': pose,\n",
    "        'eye_pose_params': eye,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89888cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ffmpeg (libx264) for H.264 encoding\n",
      "Rendering 425 frames (FLAME fusion=False)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 9c33b2f Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 9.3.0 (crosstool-NG 1.24.0.133_b0863d8_dirty)\n",
      "  configuration: --prefix=/root/miniconda3/envs/fasttalk --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --enable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1627813612080/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "/mnt/fasttalk_upperbody/renderer/util.py:53: UserWarning: Using torch.cross without specifying the dim arg is deprecated.\n",
      "Please either pass the dim explicitly or simply use torch.linalg.cross.\n",
      "The default value of dim will change to agree with that of linalg.cross in a future release. (Triggered internally at /pytorch/aten/src/ATen/native/Cross.cpp:62.)\n",
      "  torch.cross(vertices_faces[:, 2] - vertices_faces[:, 1], vertices_faces[:, 0] - vertices_faces[:, 1]))\n",
      "Input #0, rawvideo, from 'pipe:':\n",
      "  Duration: N/A, start: 0.000000, bitrate: 384000 kb/s\n",
      "    Stream #0:0: Video: rawvideo (RGB[24] / 0x18424752), rgb24, 800x800, 384000 kb/s, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (rawvideo (native) -> h264 (libx264))\n",
      "[libx264 @ 0x55d85bc5e1c0] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x55d85bc5e1c0] profile High, level 3.1, 4:2:0, 8-bit\n",
      "[libx264 @ 0x55d85bc5e1c0] 264 - core 161 r3030M 8bd6d28 - H.264/MPEG-4 AVC codec - Copyleft 2003-2020 - http://www.videolan.org/x264.html - options: cabac=1 ref=1 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=2 psy=1 psy_rd=1.00:0.00 mixed_ref=0 me_range=16 chroma_me=1 trellis=0 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=0 threads=25 lookahead_threads=6 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=1 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=10 rc=crf mbtree=1 crf=18.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to '/mnt/fasttalk_upperbody/demo/smplx_flame_fused.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 800x800, q=-1--1, 25 fps, 12800 tbn, 25 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 5/425 frames\n",
      "  Rendered 10/425 frames\n",
      "  Rendered 15/425 frames\n",
      "  Rendered 20/425 frames\n",
      "  Rendered 15/425 frames\n",
      "  Rendered 20/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   16 fps=0.0 q=0.0 size=       0kB time=00:00:00.00 bitrate=N/A speed=   0x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 25/425 frames\n",
      "  Rendered 30/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   32 fps= 31 q=0.0 size=       0kB time=00:00:00.00 bitrate=N/A speed=   0x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 35/425 frames\n",
      "  Rendered 40/425 frames\n",
      "  Rendered 45/425 frames\n",
      "  Rendered 50/425 frames\n",
      "  Rendered 45/425 frames\n",
      "  Rendered 50/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   49 fps= 31 q=23.0 size=       0kB time=00:00:00.28 bitrate=   1.4kbits/s speed=0.179x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 55/425 frames\n",
      "  Rendered 60/425 frames\n",
      "  Rendered 65/425 frames\n",
      "  Rendered 70/425 frames\n",
      "  Rendered 65/425 frames\n",
      "  Rendered 70/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   65 fps= 31 q=23.0 size=       0kB time=00:00:00.92 bitrate=   0.4kbits/s speed=0.442x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 75/425 frames\n",
      "  Rendered 80/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   81 fps= 31 q=23.0 size=       0kB time=00:00:01.56 bitrate=   0.2kbits/s speed=0.602x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 85/425 frames\n",
      "  Rendered 90/425 frames\n",
      "  Rendered 95/425 frames\n",
      "  Rendered 100/425 frames\n",
      "  Rendered 95/425 frames\n",
      "  Rendered 100/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=   97 fps= 31 q=23.0 size=       0kB time=00:00:02.20 bitrate=   0.2kbits/s speed=0.711x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 105/425 frames\n",
      "  Rendered 110/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  114 fps= 32 q=23.0 size=       0kB time=00:00:02.88 bitrate=   0.1kbits/s speed=0.799x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 115/425 frames\n",
      "  Rendered 120/425 frames\n",
      "  Rendered 125/425 frames\n",
      "  Rendered 130/425 frames\n",
      "  Rendered 125/425 frames\n",
      "  Rendered 130/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  131 fps= 32 q=23.0 size=     256kB time=00:00:03.56 bitrate= 589.2kbits/s speed=0.865x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 135/425 frames\n",
      "  Rendered 140/425 frames\n",
      "  Rendered 145/425 frames\n",
      "  Rendered 150/425 frames\n",
      "  Rendered 145/425 frames\n",
      "  Rendered 150/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  148 fps= 32 q=23.0 size=     256kB time=00:00:04.24 bitrate= 494.7kbits/s speed=0.913x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 155/425 frames\n",
      "  Rendered 160/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  164 fps= 32 q=23.0 size=     256kB time=00:00:04.88 bitrate= 429.8kbits/s speed=0.946x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 165/425 frames\n",
      "  Rendered 170/425 frames\n",
      "  Rendered 175/425 frames\n",
      "  Rendered 180/425 frames\n",
      "  Rendered 175/425 frames\n",
      "  Rendered 180/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  181 fps= 32 q=23.0 size=     256kB time=00:00:05.56 bitrate= 377.2kbits/s speed=0.979x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 185/425 frames\n",
      "  Rendered 190/425 frames\n",
      "  Rendered 195/425 frames\n",
      "  Rendered 200/425 frames\n",
      "  Rendered 195/425 frames\n",
      "  Rendered 200/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  199 fps= 32 q=23.0 size=     512kB time=00:00:06.28 bitrate= 667.9kbits/s speed=1.01x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 205/425 frames\n",
      "  Rendered 210/425 frames\n",
      "  Rendered 215/425 frames\n",
      "  Rendered 220/425 frames\n",
      "  Rendered 215/425 frames\n",
      "  Rendered 220/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  218 fps= 33 q=23.0 size=     512kB time=00:00:07.04 bitrate= 595.8kbits/s speed=1.05x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 225/425 frames\n",
      "  Rendered 230/425 frames\n",
      "  Rendered 235/425 frames\n",
      "  Rendered 240/425 frames\n",
      "  Rendered 235/425 frames\n",
      "  Rendered 240/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  238 fps= 33 q=23.0 size=     512kB time=00:00:07.84 bitrate= 535.0kbits/s speed=1.09x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 245/425 frames\n",
      "  Rendered 250/425 frames\n",
      "  Rendered 255/425 frames\n",
      "  Rendered 260/425 frames\n",
      "  Rendered 255/425 frames\n",
      "  Rendered 260/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  258 fps= 33 q=23.0 size=     512kB time=00:00:08.64 bitrate= 485.5kbits/s speed=1.12x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 265/425 frames\n",
      "  Rendered 270/425 frames\n",
      "  Rendered 275/425 frames\n",
      "  Rendered 280/425 frames\n",
      "  Rendered 275/425 frames\n",
      "  Rendered 280/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  278 fps= 34 q=23.0 size=     768kB time=00:00:09.44 bitrate= 666.5kbits/s speed=1.15x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 285/425 frames\n",
      "  Rendered 290/425 frames\n",
      "  Rendered 295/425 frames\n",
      "  Rendered 300/425 frames\n",
      "  Rendered 295/425 frames\n",
      "  Rendered 300/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  299 fps= 34 q=23.0 size=     768kB time=00:00:10.28 bitrate= 612.0kbits/s speed=1.18x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 305/425 frames\n",
      "  Rendered 310/425 frames\n",
      "  Rendered 315/425 frames\n",
      "  Rendered 320/425 frames\n",
      "  Rendered 315/425 frames\n",
      "  Rendered 320/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  320 fps= 35 q=23.0 size=     768kB time=00:00:11.12 bitrate= 565.8kbits/s speed= 1.2x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 325/425 frames\n",
      "  Rendered 330/425 frames\n",
      "  Rendered 335/425 frames\n",
      "  Rendered 340/425 frames\n",
      "  Rendered 335/425 frames\n",
      "  Rendered 340/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  340 fps= 35 q=23.0 size=     768kB time=00:00:11.92 bitrate= 527.8kbits/s speed=1.22x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 345/425 frames\n",
      "  Rendered 350/425 frames\n",
      "  Rendered 355/425 frames\n",
      "  Rendered 360/425 frames\n",
      "  Rendered 355/425 frames\n",
      "  Rendered 360/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  361 fps= 35 q=23.0 size=    1024kB time=00:00:12.76 bitrate= 657.4kbits/s speed=1.24x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 365/425 frames\n",
      "  Rendered 370/425 frames\n",
      "  Rendered 375/425 frames\n",
      "  Rendered 380/425 frames\n",
      "  Rendered 375/425 frames\n",
      "  Rendered 380/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  378 fps= 35 q=23.0 size=    1024kB time=00:00:13.44 bitrate= 624.2kbits/s speed=1.24x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 385/425 frames\n",
      "  Rendered 390/425 frames\n",
      "  Rendered 395/425 frames\n",
      "  Rendered 400/425 frames\n",
      "  Rendered 395/425 frames\n",
      "  Rendered 400/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  397 fps= 35 q=23.0 size=    1024kB time=00:00:14.20 bitrate= 590.8kbits/s speed=1.25x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 405/425 frames\n",
      "  Rendered 410/425 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  414 fps= 35 q=23.0 size=    1024kB time=00:00:14.88 bitrate= 563.8kbits/s speed=1.26x    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rendered 415/425 frames\n",
      "  Rendered 420/425 frames\n",
      "  Rendered 425/425 frames\n",
      "✓ Saved video to /mnt/fasttalk_upperbody/demo/smplx_flame_fused.mp4\n",
      "  Rendered 425/425 frames\n",
      "✓ Saved video to /mnt/fasttalk_upperbody/demo/smplx_flame_fused.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame=  425 fps= 35 q=-1.0 Lsize=    1364kB time=00:00:16.88 bitrate= 661.9kbits/s speed=1.37x    \n",
      "video:1358kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.427863%\n",
      "[libx264 @ 0x55d85bc5e1c0] frame I:2     Avg QP: 6.37  size: 13450\n",
      "[libx264 @ 0x55d85bc5e1c0] frame P:107   Avg QP:16.17  size:  5975\n",
      "[libx264 @ 0x55d85bc5e1c0] frame B:316   Avg QP:19.37  size:  2290\n",
      "[libx264 @ 0x55d85bc5e1c0] consecutive B-frames:  0.7%  0.5%  0.0% 98.8%\n",
      "[libx264 @ 0x55d85bc5e1c0] mb I  I16..4: 75.7% 12.3% 12.0%\n",
      "[libx264 @ 0x55d85bc5e1c0] mb P  I16..4:  3.2%  4.0%  1.9%  P16..4:  6.8%  5.6%  3.0%  0.0%  0.0%    skip:75.5%\n",
      "[libx264 @ 0x55d85bc5e1c0] mb B  I16..4:  0.5%  0.2%  0.1%  B16..8:  8.1%  4.3%  0.8%  direct: 2.7%  skip:83.3%  L0:46.3% L1:42.3% BI:11.4%\n",
      "[libx264 @ 0x55d85bc5e1c0] 8x8 transform intra:36.8% inter:7.3%\n",
      "[libx264 @ 0x55d85bc5e1c0] coded y,uvDC,uvAC intra: 39.4% 59.8% 34.3% inter: 2.2% 4.6% 0.9%\n",
      "[libx264 @ 0x55d85bc5e1c0] i16 v,h,dc,p: 74%  8%  6% 12%\n",
      "[libx264 @ 0x55d85bc5e1c0] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 46% 10% 10%  4%  6%  9%  5%  7%  3%\n",
      "[libx264 @ 0x55d85bc5e1c0] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 27% 14% 16%  6% 10%  9%  9%  6%  4%\n",
      "[libx264 @ 0x55d85bc5e1c0] i8c dc,h,v,p: 40% 11% 36% 13%\n",
      "[libx264 @ 0x55d85bc5e1c0] Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "[libx264 @ 0x55d85bc5e1c0] kb/s:654.09\n"
     ]
    }
   ],
   "source": [
    "# Render full sequence to MP4 using the in-repo Renderer class + optional FLAME head fusion\n",
    "import os, sys, subprocess, shutil\n",
    "class SMPLXRenderer:\n",
    "    def __init__(self, image_size=800):\n",
    "        from renderer.renderer import Renderer\n",
    "        self.rasterize_fn = Renderer.rasterize\n",
    "        self.add_directionlight_fn = Renderer.add_directionlight\n",
    "        self.image_size = image_size\n",
    "    def render_smplx(self, vertices, cam_params, faces, device):\n",
    "        from renderer.util import vertex_normals, face_vertices\n",
    "        batch_size = vertices.shape[0]\n",
    "        transformed_vertices = batch_orth_proj(vertices, cam_params)\n",
    "        light_positions = torch.tensor([\n",
    "            [-1, -1, -1], [1, -1, -1], [-1, +1, -1], [1, +1, -1], [0, 0, -1]\n",
    "        ])[None, :, :].expand(batch_size, -1, -1).float()\n",
    "        light_intensities = torch.ones_like(light_positions).float() * 1.7\n",
    "        lights = torch.cat((light_positions, light_intensities), 2).to(device)\n",
    "        transformed_vertices = transformed_vertices.clone()\n",
    "        transformed_vertices[:, :, 2] = transformed_vertices[:, :, 2] + 10\n",
    "        normals = vertex_normals(vertices, faces)\n",
    "        face_normals = face_vertices(normals, faces)\n",
    "        colors = torch.tensor([12, 156, 91])[None, None, :].repeat(1, vertices.shape[1], 1).float() / 255.0\n",
    "        colors = colors.to(device)\n",
    "        face_colors = face_vertices(colors, faces[0:1] if faces.shape[0] == 1 else faces)\n",
    "        face_colors = face_colors.expand(batch_size, -1, -1, -1)\n",
    "        attributes = torch.cat([face_colors, face_normals], -1)\n",
    "        rendering = self.rasterize_fn(self, transformed_vertices, faces, attributes)\n",
    "        albedo_images = rendering[:, :3, :, :]\n",
    "        normal_images = rendering[:, 3:6, :, :]\n",
    "        shading = self.add_directionlight_fn(self, normal_images.permute(0, 2, 3, 1).reshape([batch_size, -1, 3]), lights)\n",
    "        shading_images = shading.reshape([batch_size, albedo_images.shape[2], albedo_images.shape[3], 3]).permute(0, 3, 1, 2).contiguous()\n",
    "        shaded_images = albedo_images * shading_images\n",
    "        return shaded_images\n",
    "\n",
    "# Guard: make FLAME fusion optional if setup cell wasn't run\n",
    "fusion_enabled = bool(globals().get('fusion_enabled', False))\n",
    "smplx_idx_t = globals().get('smplx_idx_t', None)\n",
    "flame_idx_t = globals().get('flame_idx_t', None)\n",
    "if fusion_enabled and (smplx_idx_t is None or flame_idx_t is None):\n",
    "    print('WARN: FLAME mapping tensors not found in session; disabling fusion.')\n",
    "    fusion_enabled = False\n",
    "\n",
    "renderer = SMPLXRenderer(image_size=800)\n",
    "demo_path = '/mnt/fasttalk_upperbody/demo'\n",
    "os.makedirs(demo_path, exist_ok=True)\n",
    "out_video_renderer = os.path.join(demo_path, 'smplx_flame_fused.mp4')\n",
    "fps = 25\n",
    "res = 800\n",
    "use_ffmpeg = shutil.which('ffmpeg') is not None\n",
    "ffmpeg_proc = None\n",
    "if use_ffmpeg:\n",
    "    print('Using ffmpeg (libx264) for H.264 encoding')\n",
    "    ffmpeg_cmd = ['ffmpeg','-y','-f','rawvideo','-vcodec','rawvideo','-pix_fmt','rgb24','-s',f'{res}x{res}','-r',str(fps),'-i','-','-an','-vcodec','libx264','-pix_fmt','yuv420p','-preset','veryfast','-crf','18',out_video_renderer]\n",
    "    ffmpeg_proc = subprocess.Popen(ffmpeg_cmd, stdin=subprocess.PIPE)\n",
    "else:\n",
    "    print('ffmpeg not found; falling back to OpenCV (mp4v)')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(out_video_renderer, fourcc, fps, (res, res), True)\n",
    "    if not writer.isOpened(): raise RuntimeError('OpenCV VideoWriter init failed')\n",
    "print(f'Rendering {len(frame_keys)} frames (FLAME fusion={fusion_enabled})...')\n",
    "with torch.no_grad():\n",
    "    for idx, fk in enumerate(frame_keys):  # slice e.g., frame_keys[:10] for a quick preview\n",
    "        coeffs = parse_frame(tracking[fk])\n",
    "        flame_coeffs = parse_flame(tracking[fk]) if fusion_enabled else None\n",
    "        # SMPL-X full body\n",
    "        smplx_out = model(betas=betas_used.to(device),\n",
    "                          global_orient=coeffs['global_orient'].to(device),\n",
    "                          body_pose=coeffs['body_pose'].to(device),\n",
    "                          jaw_pose=coeffs['jaw_pose'].to(device),\n",
    "                          leye_pose=coeffs['leye_pose'].to(device),\n",
    "                          reye_pose=coeffs['reye_pose'].to(device),\n",
    "                          left_hand_pose=coeffs['left_hand_pose'].to(device),\n",
    "                          right_hand_pose=coeffs['right_hand_pose'].to(device),\n",
    "                          expression=coeffs['expression'].to(device),\n",
    "                          transl=coeffs['transl'].to(device))\n",
    "        verts_body = smplx_out.vertices  # [1, Vb, 3]\n",
    "        if fusion_enabled and flame_coeffs is not None and (smplx_idx_t is not None) and (flame_idx_t is not None):\n",
    "            f_out = flame(shape_params=flame_coeffs['shape_params'].to(device),\n",
    "                          expression_params=flame_coeffs['expression_params'].to(device),\n",
    "                          pose_params=flame_coeffs['pose_params'].to(device),\n",
    "                          eye_pose_params=flame_coeffs['eye_pose_params'].to(device))\n",
    "            verts_flame = f_out[0] if isinstance(f_out, tuple) else f_out  # [1, Vf, 3] or [Vf,3]\n",
    "            if verts_flame.ndim == 2: verts_flame = verts_flame.unsqueeze(0)\n",
    "            # Replace SMPL-X head vertices with FLAME ones via mapping\n",
    "            try:\n",
    "                verts_fused = verts_body.clone()\n",
    "                verts_fused[:, smplx_idx_t, :] = verts_flame[:, flame_idx_t, :]\n",
    "            except Exception as e:\n",
    "                if idx == 0: print('Fusion error, disabling fusion:', e)\n",
    "                fusion_enabled = False\n",
    "                verts_fused = verts_body\n",
    "        else:\n",
    "            verts_fused = verts_body\n",
    "        # Center relative to initial frame\n",
    "        img_t = renderer.render_smplx(verts_fused - center_t, cam_params, faces_t, device)\n",
    "        img_rgb = (img_t[0].detach().cpu().permute(1, 2, 0).numpy().clip(0, 1) * 255).astype(np.uint8)\n",
    "        if use_ffmpeg: ffmpeg_proc.stdin.write(img_rgb.tobytes())\n",
    "        else: writer.write(img_rgb[:, :, ::-1])\n",
    "        if (idx + 1) % 5 == 0 or (idx + 1) == len(frame_keys):\n",
    "            print(f'  Rendered {idx + 1}/{len(frame_keys)} frames')\n",
    "if use_ffmpeg:\n",
    "    ffmpeg_proc.stdin.close(); ffmpeg_proc.wait()\n",
    "else:\n",
    "    writer.release()\n",
    "print(f'✓ Saved video to {out_video_renderer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ca899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ef8336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c4334d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'frame': 'frame_000000', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000001', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000002', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000003', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000004', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000005', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000006', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "{'frame': 'frame_000007', 'smplx_left_hand_pose_shape': (15, 3), 'smplx_right_hand_pose_shape': (15, 3), 'mano_left_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_right_keys': ['pred_cam', 'global_orient', 'hand_pose', 'pred_cam_t', 'focal_length', 'camera_RT_params'], 'mano_left_axisang_shape': (), 'mano_right_axisang_shape': (), 'mano_left_generic_shape': (1, 15, 3, 3), 'mano_right_generic_shape': (1, 15, 3, 3)}\n",
      "\n",
      "First frame left_hand_pose length: 45\n",
      "First frame right_hand_pose length: 45\n",
      "First frame left_hand_pose first 9 vals: [-0.08508328  0.12008699 -1.3282686   0.20950751 -0.20685533 -0.6650091\n",
      "  0.10189541 -0.16178168 -0.41195333]\n",
      "First frame right_hand_pose first 9 vals: [ 0.00409186 -0.321227    0.8929021   0.41290188  0.21008031  1.1836927\n",
      " -0.23453547  0.15982942  0.46926707]\n"
     ]
    }
   ],
   "source": [
    "# Inspect hand-related coefficients across a few frames to diagnose parsing issues\n",
    "frames_to_check = frame_keys[:8]\n",
    "report = []\n",
    "for fk in frames_to_check:\n",
    "    fr = tracking[fk]\n",
    "    smplx_part = fr.get('smplx_coeffs', {})\n",
    "    l_mano = fr.get('left_mano_coeffs', {})\n",
    "    r_mano = fr.get('right_mano_coeffs', {})\n",
    "    def shape_of(x):\n",
    "        try:\n",
    "            arr = np.asarray(x)\n",
    "            return tuple(arr.shape), arr.flatten()[:6].tolist()\n",
    "        except Exception:\n",
    "            return None, None\n",
    "    entry = {\n",
    "        'frame': fk,\n",
    "        'smplx_left_hand_pose_shape': shape_of(smplx_part.get('left_hand_pose'))[0],\n",
    "        'smplx_right_hand_pose_shape': shape_of(smplx_part.get('right_hand_pose'))[0],\n",
    "        'mano_left_keys': list(l_mano.keys()) if isinstance(l_mano, dict) else None,\n",
    "        'mano_right_keys': list(r_mano.keys()) if isinstance(r_mano, dict) else None,\n",
    "        'mano_left_axisang_shape': shape_of(l_mano.get('hand_pose_axisang'))[0],\n",
    "        'mano_right_axisang_shape': shape_of(r_mano.get('hand_pose_axisang'))[0],\n",
    "        'mano_left_generic_shape': shape_of(l_mano.get('hand_pose'))[0],\n",
    "        'mano_right_generic_shape': shape_of(r_mano.get('hand_pose'))[0],\n",
    "    }\n",
    "    report.append(entry)\n",
    "\n",
    "for r in report:\n",
    "    print(r)\n",
    "\n",
    "# Also print a single example of the raw left/right hand arrays for first frame for deeper look\n",
    "first_frame = tracking[frame_keys[0]]\n",
    "lf_raw = first_frame.get('smplx_coeffs', {}).get('left_hand_pose')\n",
    "rf_raw = first_frame.get('smplx_coeffs', {}).get('right_hand_pose')\n",
    "print('\\nFirst frame left_hand_pose length:', None if lf_raw is None else len(np.asarray(lf_raw).reshape(-1)))\n",
    "print('First frame right_hand_pose length:', None if rf_raw is None else len(np.asarray(rf_raw).reshape(-1)))\n",
    "print('First frame left_hand_pose first 9 vals:', None if lf_raw is None else np.asarray(lf_raw).reshape(-1)[:9])\n",
    "print('First frame right_hand_pose first 9 vals:', None if rf_raw is None else np.asarray(rf_raw).reshape(-1)[:9])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttalk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
